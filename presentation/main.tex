\documentclass[xcolor=dvipsnames]{beamer}

\input{utils/packages.tex}
\input{utils/bindings.tex}
\input{contents/titlepage.tex}



\begin{document}
    \begin{frame}
        \titlepage
    \end{frame}
    \begin{frame}{Contribution}
        This work was developed as the final project for the \textbf{Evgeniy Burnaev}'s course  
        \textbf{Machine Learning} that was held in Skolkovo Intsitute of Science and Technology.
        Contribution of all the participants is as follows:
        \begin{enumerate}
            \item \textbf{Anton Bolychev} participated as the \textbf{Principal Developer} in the present work. The core
            research, the code, the repo structure was prepared and finalized by him. Moreover he is the only 
            author of the presentation.
            \item \textbf{Georgiy Malaniya} particapated as the \textbf{Team Lead}. He maintained all the management
            process of all the participants
            \item \textbf{Oleg Shepelin} and \textbf{Nikolay Kalmykov} participated as 
            the \textbf{Core Developers} and prepared the draft of main loop for Langevin Dynamics
            \item \textbf{Vadim Shirokinsky} was responsible for the calculating FIDs routine and took part in preparing
            the presentation for Skoltech.
            \item \textbf{Anastasiya Archangelskaya} finalized results and prepared the report with presentation for
            Skoltech
        \end{enumerate}
    \end{frame}
    \section{GAN as Energy-Based Model}
    \label{sec:energy-gan} 
    \begin{frame}{GAN}
        \begin{align*}
            L_D &= -\mathbb{E}_{x \sim p_{data}} [ \text{log} D(x)] - \mathbb{E}_{z \sim p_z} [\text{log}(1 - D(G(z))] \\    
            L_G &= -\mathbb{E}_{z \sim p_z} [\text{log} D(G(z))]
        \end{align*}
    \end{frame}
    
    \begin{frame}{GAN as Energy Based Model}
        Assume that Discriminator is suboptimal, i.e. $D = D^*$
        \begin{equation*}
            D(x) = \operatorname*{logit}(d(x)) = \frac{1}{1 + \exp(-d(x))} \approx \frac{p_d(x)}{p_d(x) + p_g(x)}  = \frac{1}{1 + p_g(x)/p_d(x)}
        \end{equation*}
        Thus,
        \begin{equation*}
                p_d^*(x) = p_g(x) e^{d(x)} / K = \exp\left( - (-\log p_g(x) - d(x))\right) / K
        \end{equation*}
        Energy Function. Boltzmann distribution
        \begin{equation*}
            p(z) = \exp(-E(z)) / K 
        \end{equation*}
        Thus, assuming that $x = G(z)$ one can obtain the following equation for Energy for GAN
        \begin{equation*}
            E(z) = -\log p_0(z) - d(G(z))
        \end{equation*}
    \end{frame}

    \begin{frame}{Langevin Dynamics}
        \begin{equation*}
            z_{i+1} = z_i - \epsilon/2 \nabla_z E(z) + \sqrt{\epsilon} n, n \sim N(0, I)
        \end{equation*}

        \begin{center}
         \begin{algorithmic}
            \STATE {\bfseries Input:} N$\in \mathbf{N}_+$, $\epsilon > 0$ 
            \STATE {\bfseries Output:} Latent code $z_N \sim p_t(z)$
            \STATE Sample $z_0 \sim p_0(z)$   
            \FOR{$i=1$ {\bfseries to} $N$}
            \STATE $n_i \sim N(0, 1)$
            \STATE $z_{i + 1} = z_i - \epsilon/2 \nabla_z E(z_i) + \sqrt{\epsilon} n_i$
            \ENDFOR
         \end{algorithmic}
        \end{center}
    \end{frame}
    \begin{frame}{Frechet Inception Distance}
        FID can be calculated according to the following formula
        \begin{equation*}
            d_F(\mu, \nu):=\left(\inf _{\gamma \in \Gamma(\mu, \nu)} \int_{\mathbb{R}^n \times \mathbb{R}^n}\|x-y\|^2 \mathrm{~d} \gamma(x, y)\right)^{1 / 2}
        \end{equation*}
    \end{frame}
    \begin{frame}{\hypertarget{frame:res-energy}{Results}} 
        \begin{center}
            If one apply Langevin Dynamics for pretrained DCGAN on CIFAR10 one can observe 
            that FID metrics decreases with increasing number of Langevin steps.
            \includegraphics[width=0.7\textwidth]{pics/FID_EBMGAN.png}
        \end{center}
    \end{frame}
    \section{Score-based Generative Modelling through SDE}
    \label{sec:reverse-sde} 
    \begin{frame}{Score-based Generative Modelling through SDE}
        \begin{center}
            The core idea of the paper can be described via the following picture
            \includegraphics[width=0.7\textwidth]{pics/NoiseDenoise.png}
        \end{center}
    \end{frame}

    \begin{frame}{Score-based Generative Modelling through SDE}
        \begin{center}
            \includegraphics[width=0.7\textwidth]{pics/NoiseDenoise2.png}
        \end{center}
        The main problem is to fit the neural network $\mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x}(t), t)$ such that 
        $$
        \mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x}(t), t) \approx \nabla_{\mathbf{x}} \log p_t(\mathbf{x})
        $$
    \end{frame}

    \begin{frame}{Loss function}
        \begin{multline*}
            \boldsymbol{\theta}^*=\underset{\boldsymbol{\theta}}{\arg \min } \\ \mathbb{E}_t\left\{ 
            \lambda(t) \mathbb{E}_{\mathbf{x}(0)} \mathbb{E}_{\mathbf{x}(t) \mid \mathbf{x}(0)}\left[\left\|\mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x}(t), t)-\nabla_{\mathbf{x}(t)} \log p_{0 t}(\mathbf{x}(t) \mid \mathbf{x}(0))\right\|_2^2\right]\right\} .
        \end{multline*}
        
        where 
        $$
        \lambda \propto 1 / \mathbb{E}\left[\left\|\nabla_{\mathbf{x}(t)} \log p_{0 t}(\mathbf{x}(t) \mid \mathbf{x}(0))\right\|_2^2\right]
        $$
    \end{frame}

    \begin{frame}{2 approaches}
        In $\mathrm{d} \mathbf{x}=\mathbf{f}(\mathbf{x}, t) \mathrm{d} t+g(t) \mathrm{d} \mathbf{w}$ functions $\mathbf{f}$ and $g$ can be arbitrary, so we will consider 2 approaches
        \begin{itemize}
            \item Variance Exploding Approach
            \item Variance Preserving Approach
        \end{itemize}
    \end{frame}

    \begin{frame}{Variance Exploding}
        \begin{itemize}
            \item SDE 
        $$
        \mathrm{d} \mathbf{x}=\sqrt{\frac{\mathrm{d}\left[\sigma^2(t)\right]}{\mathrm{d} t}} \mathrm{~d} \mathbf{w}
        $$
            \item Forward Sampling
            $$
            \mathbf{x}_i=\mathbf{x}_{i-1}+\sqrt{\sigma_i^2-\sigma_{i-1}^2} \mathbf{z}_{i-1}
            $$
        \end{itemize}
    \end{frame}


    \begin{frame}{Variance Preserving}
        \begin{itemize}
        \item SDE 
            $$
        \mathrm{d} \mathbf{x}=-\frac{1}{2} \beta(t) \mathbf{x} \mathrm{d} t+\sqrt{\beta(t)} \mathrm{d} \mathbf{w}
        $$
        \item Forward Sampling
        $$
        \mathbf{x}_i=\sqrt{1-\beta_i} \mathbf{x}_{i-1}+\sqrt{\beta_i} \mathbf{z}_{i-1}
        $$
        \end{itemize}
    \end{frame}

    \begin{frame}{Ancestral Sampling for Variance Preserving}
        $$
        \mathbf{x}_{i-1}=\frac{1}{\sqrt{1-\beta_i}}\left(\mathbf{x}_i+\beta_i \mathbf{s}_{\boldsymbol{\theta}^*}\left(\mathbf{x}_i, i\right)\right)+\sqrt{\beta_i} \mathbf{z}_i, \quad i=N, N-1, \cdots, 1
        $$
    \end{frame}

    \begin{frame}{Reverse Diffusion}
        Given a forward SDE
$$
\mathrm{d} \mathbf{x}=\mathbf{f}(\mathbf{x}, t) \mathrm{d} t+\mathbf{G}(t) \mathrm{d} \mathbf{w}
$$
and suppose the following iteration rule is a discretization of it:
$$
\mathbf{x}_{i+1}=\mathbf{x}_i+\mathbf{f}_i\left(\mathbf{x}_i\right)+\mathbf{G}_i \mathbf{z}_i, \quad i=0,1, \cdots, N-1
$$
Thus, one can propose to discretize the reverse-time SDE
$$
\mathrm{d} \mathbf{x}=\left[\mathbf{f}(\mathbf{x}, t)-\mathbf{G}(t) \mathbf{G}(t)^{\top} \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right] \mathrm{d} t+\mathbf{G}(t) \mathrm{d} \overline{\mathbf{w}}
$$
which gives the following iteration rule for $i \in\{0,1, \cdots, N-1\}$ :
$$
\mathbf{x}_i=\mathbf{x}_{i+1}-\mathbf{f}_{i+1}\left(\mathbf{x}_{i+1}\right)+\mathbf{G}_{i+1} \mathbf{G}_{i+1}^{\top} \mathbf{s}_{\boldsymbol{\theta}^*}\left(\mathbf{x}_{i+1}, i+1\right)+\mathbf{G}_{i+1} \mathbf{z}_{i+1},
$$
where our trained score-based model $\mathbf{s}_{\boldsymbol{\theta}} *\left(\mathbf{x}_i, i\right)$.
    \end{frame}

    \begin{frame}{Predictor-Corrector Sampling}
        \begin{center}
            \includegraphics[width=\textwidth]{pics/PC.png}
        \end{center}
    \end{frame}

    \begin{frame}{\hypertarget{frame:res-ve}{Results. FID on CIFAR10. VE}}            
        \begin{center}
            \includegraphics[width=0.7\textwidth]{pics/VE_table.png}
        \end{center}
        As we can see Predictor-Corrector sampling gives the best performance in terms of
        FID metrics
    \end{frame}


    \begin{frame}{\hypertarget{frame:res-vp}{Results. FID on CIFAR10. VP}}            
        \begin{center}
            \includegraphics[width=0.7\textwidth]{pics/VP_table.png}
        \end{center}
        As we can see Predictor-Corrector sampling gives the best performance in terms of
        FID metrics
    \end{frame}
    \section{Conclusion}
    \begin{frame}{Conclusion}
        This work is devoted to examining the power of SDE theory in Image Generation.
        The work consists of 2 parts
        \begin{enumerate}
            \item The \ref{sec:energy-gan} part examines the paper [\ref{bib:energy-gan}] which main idea is based on the fact
            that GAN can be interpreted as Energy Based Model. Thus, one can easily
            apply Langevin Dynamics for it which can improve FID metrics for already pretrained 
            GAN model. The corresponding plot of FID behavior with dependence on  
            the Langevin steps is presented on \textcolor{blue}{\hyperlink{frame:res-energy}{this}} slide.
            \item The \ref{sec:reverse-sde} part reproduces experiments from the paper 
            [\ref{bib:reverse-sde}] that implements the following 
            idea. What if one can controllably transform the initial data disctibution into
            noise and reverse the process? Well, that can be done via reversing the SDE formula. Namely, 
            if one consider the SDE process that is defined by SDE 
            $\mathrm{d} \mathbf{x}=\mathbf{f}(\mathbf{x}, t) \mathrm{d} t+g(t) \mathrm{d} \mathbf{w}$
            then one can reverse it via 
            $\mathrm{d} \mathbf{x}=\left[\mathbf{f}(\mathbf{x}, t)-g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right] \mathrm{d} t+g(t) \mathrm{d} \overline{\mathbf{w}}$. 
            In the paper several sampling technics are examined, and the results are finalized on 
            \textcolor{blue}{\hyperlink{frame:res-ve}{this}} and 
            \textcolor{blue}{\hyperlink{frame:res-vp}{this}} slides. 
        \end{enumerate}
    \end{frame}
    \section{References}

    \begin{frame}{References}
        \begin{enumerate}
            \item\label{bib:energy-gan} Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, Yoshua Bengio. \textit{Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling}  \url{https://arxiv.org/abs/2003.06060}
            \item\label{bib:reverse-sde} Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. \textit{Score-Based Generative Modeling through Stochastic Differential Equations} \url{https://arxiv.org/abs/2011.13456}            
            \item Github for DCGAN on CIFAR10 \url{https://github.com/csinva/gan-vae-pretrained-pytorch}
        \end{enumerate}
    \end{frame}

    % \input{contents/introduction.tex}
    % \input{contents/statement.tex}
    % \input{contents/concepts.tex}
    % \input{contents/global_stabilization.tex}
    
    % \bibliography{references}
    % \bibliographystyle{ieeetr}
\end{document}